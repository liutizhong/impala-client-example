/**
 * Autogenerated by Thrift Compiler (0.9.3)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package com.cloudera.impala.thrift;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import org.apache.thrift.async.AsyncMethodCallback;
import org.apache.thrift.server.AbstractNonblockingServer.*;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import javax.annotation.Generated;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
public class ErrorCodesConstants {

  public static final List<String> TErrorMessage = new ArrayList<String>();
  static {
    TErrorMessage.add("");
    TErrorMessage.add("<UNUSED>");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("$0");
    TErrorMessage.add("Parquet files should not be split into multiple hdfs-blocks. file=$0");
    TErrorMessage.add("Column metadata states there are $0 values, but read $1 values from column $2. file=$3");
    TErrorMessage.add("(unused)");
    TErrorMessage.add("ParquetScanner: reached EOF while deserializing data page header. file=$0");
    TErrorMessage.add("Metadata states that in group $0($1) there are $2 rows, but $3 rows were read.");
    TErrorMessage.add("(unused)");
    TErrorMessage.add("File '$0' column '$1' does not have the decimal precision set.");
    TErrorMessage.add("File '$0' column '$1' has a precision that does not match the table metadata  precision. File metadata precision: $2, table metadata precision: $3.");
    TErrorMessage.add("File '$0' column '$1' does not have converted type set to DECIMAL");
    TErrorMessage.add("File '$0' column '$1' contains decimal data but the table metadata has type $2");
    TErrorMessage.add("Problem parsing file $0 at $1$2");
    TErrorMessage.add("Decompressor: block size is too big.  Data is likely corrupt. Size: $0");
    TErrorMessage.add("Decompressor: invalid compressed length.  Data is likely corrupt.");
    TErrorMessage.add("Snappy: GetUncompressedLength failed");
    TErrorMessage.add("SnappyBlock: RawUncompress failed");
    TErrorMessage.add("Snappy: Decompressed size is not correct.");
    TErrorMessage.add("Unknown disk id.  This will negatively affect performance. Check your hdfs settings to enable block location metadata.");
    TErrorMessage.add("Reserved resource size ($0) is larger than query mem limit ($1), and will be restricted to $1. Configure the reservation size by setting RM_INITIAL_MEM.");
    TErrorMessage.add("Cannot perform join at hash join node with id $0. The input data was partitioned the maximum number of $1 times. This could mean there is significant skew in the data or the memory limit is set too low.");
    TErrorMessage.add("Cannot perform aggregation at hash aggregation node with id $0. The input data was partitioned the maximum number of $1 times. This could mean there is significant skew in the data or the memory limit is set too low.");
    TErrorMessage.add("Builtin '$0' with symbol '$1' does not exist. Verify that all your impalads are the same version.");
    TErrorMessage.add("RPC Error: $0");
    TErrorMessage.add("RPC timed out");
    TErrorMessage.add("Failed to verify function $0 from LLVM module $1, see log for more details.");
    TErrorMessage.add("File $0 corrupt. RLE level data bytes = $1");
    TErrorMessage.add("Column '$0' has conflicting Avro decimal types. Table schema $1: $2, file schema $1: $3");
    TErrorMessage.add("Column '$0' has conflicting Avro decimal types. Declared $1: $2, $1 in table's Avro schema: $3");
    TErrorMessage.add("Unresolvable types for column '$0': table type: $1, file type: $2");
    TErrorMessage.add("Unresolvable types for column '$0': declared column type: $1, table's Avro schema type: $2");
    TErrorMessage.add("Field $0 is missing from file and default values of type $1 are not yet supported.");
    TErrorMessage.add("Inconsistent table metadata. Mismatch between column definition and Avro schema: cannot read field $0 because there are only $1 fields.");
    TErrorMessage.add("Field $0 is missing from file and does not have a default value.");
    TErrorMessage.add("Field $0 is nullable in the file schema but not the table schema.");
    TErrorMessage.add("Inconsistent table metadata. Field $0 is not a record in the Avro schema.");
    TErrorMessage.add("Could not read definition level, even though metadata states there are $0 values remaining in data page. file=$1");
    TErrorMessage.add("Mismatched number of values in column index $0 ($1 vs. $2). file=$3");
    TErrorMessage.add("Failed to decode dictionary-encoded value. file=$0");
    TErrorMessage.add("SSL private-key password command ('$0') failed with error: $1");
    TErrorMessage.add("The SSL certificate path is blank");
    TErrorMessage.add("The SSL private key path is blank");
    TErrorMessage.add("The SSL certificate file does not exist at path $0");
    TErrorMessage.add("The SSL private key file does not exist at path $0");
    TErrorMessage.add("SSL socket creation failed: $0");
    TErrorMessage.add("Memory allocation of $0 bytes failed");
    TErrorMessage.add("Could not read repetition level, even though metadata states there are $0 values remaining in data page. file=$1");
    TErrorMessage.add("File '$0' has an incompatible Parquet schema for column '$1'. Column type: $2, Parquet schema:\n$3");
    TErrorMessage.add("Failed to allocate $0 bytes for collection '$1'.\nCurrent buffer size: $2 num tuples: $3.");
    TErrorMessage.add("Temporary device for directory $0 is blacklisted from a previous error and cannot be used.");
    TErrorMessage.add("Temporary file $0 is blacklisted from a previous error and cannot be expanded.");
    TErrorMessage.add("RPC client failed to connect: $0");
    TErrorMessage.add("Metadata for file '$0' appears stale. Try running \"refresh $1\" to reload the file metadata.");
    TErrorMessage.add("File '$0' has an invalid version number: $1\nThis could be due to stale metadata. Try running \"refresh $2\".");
    TErrorMessage.add("Tried to read $0 bytes but could only read $1 bytes. This may indicate data file corruption. (file $2, byte offset: $3)");
    TErrorMessage.add("Invalid read of $0 bytes. This may indicate data file corruption. (file $1, byte offset: $2)");
    TErrorMessage.add("File '$0' has an invalid version header: $1\nMake sure the file is an Avro data file.");
    TErrorMessage.add("$0's allocations exceeded memory limits.");
    TErrorMessage.add("Cannot process row that is bigger than the IO size (row_size=$0, null_indicators_size=$1). To run this query, increase the IO size (--read_size option).");
    TErrorMessage.add("For better performance, snappy-, gzip-, and bzip-compressed files should not be split into multiple HDFS blocks. file=$0 offset $1");
    TErrorMessage.add("$0 Data error, likely data corrupted in this block.");
    TErrorMessage.add("$0 Decompressor error at $1, code=$2");
    TErrorMessage.add("Decompression failed to make progress, but end of input is not reached. File appears corrupted. file=$0");
    TErrorMessage.add("Unexpected end of compressed file. File may be truncated. file=$0");
    TErrorMessage.add("Sender timed out waiting for receiver fragment instance: $0");
    TErrorMessage.add("Kudu type $0 is not available in Impala.");
    TErrorMessage.add("Impala type $0 is not available in Kudu.");
    TErrorMessage.add("Kudu is not supported on this operating system.");
    TErrorMessage.add("Kudu features are disabled by the startup flag --disable_kudu.");
    TErrorMessage.add("Cannot perform hash join at node with id $0. Repartitioning did not reduce the size of a spilled partition. Repartitioning level $1. Number of rows $2.");
    TErrorMessage.add("Cannot perform aggregation at node with id $0. Repartitioning did not reduce the size of a spilled partition. Repartitioning level $1. Number of rows $2.");
  }

}
